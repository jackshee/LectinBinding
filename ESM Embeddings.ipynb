{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jackshee/LectinBinding/blob/main/ESM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook only runs in Google Colab to leverage GPU access. It accesses the pretrained `ESM2 model` published by Meta via the `HuggingFace` API and encodes known lectins in the glycan microarray dataset available through `glycowork` by converting an amino acid sequence representation e.g. a `String` (AACTNTSCSGHGECVETINNYTCKCDPGFSGLKCEQ) to a vector representation (protein sequence embedding). \n",
    "\n",
    "It is important to note that accessing `ESM2` this way requires custom fine tuning of the model if a new model architecture is defined. However, for our purposes, we use the pretrained `LectinOracle` model that has been *already* fine-tuned this way. \n",
    "\n",
    "For the sake of reproducibility, and further customisation, I tried retraining my own version of `LectinOracle` and got \"similar\" sets of embeddings. However there is always stochasticity from training without random seed initialised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AoasrgYr5HZz",
    "outputId": "c4053650-0873-49a3-f840-c9fd56e9de93"
   },
   "outputs": [],
   "source": [
    "# Mounting google drive, required to save embeddings for downstream analysis\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Uncomment below to install dependencies on allocated machine when connecting to new instance. \n",
    "# Note: torch comes preinstalled in Colab environment\n",
    "!pip install glycowork\n",
    "!pip install transformers\n",
    "!pip install 'glycowork[ml]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing ESM2 via Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "7f62e0c9f7d1471baca143e5b9bc343b",
      "3f4bbaec78e24fb6be24870c8cfdc86e",
      "a699143b257e429285ec2f83b458a87d",
      "0b8917f6d5dc437ebfe74c59bcaddbaa",
      "3e0db112862748e093d11e9376c71f5a",
      "24aa62e32d9e4d129575f521e082b2d0",
      "75ed37e6971642b9a00a8e2f51c582a6",
      "64c0c3836aeb40aa904c5acb22bc8a4a",
      "1c8cd33a0f9746edb5b73485f50dbae0",
      "c5cc450465ca4e0186a7a3341ab5a63d",
      "a376f5016b5741d48132676dcf0c7897",
      "e0bf0e75a43849149c7e90fd40120796",
      "3cd91283fdf2481f9e544cdc4c014bed",
      "3337d5cbcd644fc98650a1f112168776",
      "21583c220049466a975fd4733caa5698",
      "eadca3639b4348f7aae0a37f33f3cc75",
      "c34a15ad099543feacccc250271a0ceb",
      "61cb985c9a3f4b2c8ec6d66054ce0de9",
      "29a8f87213b642699f3ea37945d9c5ee",
      "488bc16bc60b4d0bbd7191c0c640b1e4",
      "89476b7e85e649ae9865fd9957fbe33a",
      "50ec24df494e405e9c1d11227c705ab7",
      "7103023aee884916b192dcb43679072f",
      "8ece8d9a21b04962b14e1ce205b05e97",
      "b8b4721ec89b482d8d25d79f04c57b13",
      "8bfce4877f7e4155a648c9714bb17404",
      "f0cdf985dd084e97b01b99cf47492ca1",
      "88a8615651b34b2197dd7f2cb75af009",
      "340c7e3d458b4c63bec2b01a5098a18e",
      "7da730be22d74f008dc266531325bcc2",
      "51ec29c64aa84b6eac6c2a06c58336f7",
      "89b681ca09274855962e00906bc51ceb",
      "dd088147daec4e18a835767032480ae3",
      "031bf2f37e6a4f0c90e4baaeb94fc9d7",
      "2498eddbe21f495e8e68dc15572ec64d",
      "cf59c57008b94c9aab6f07c96e4766cf",
      "5464926b86ff40769f4433b2db248ec0",
      "0347d309cd5f4907af23cbec2fed9891",
      "fe99f1eb19034692ba09dde8e9698cf5",
      "bf08080650fd42809d96c201c810652c",
      "22c7af03cff04eeaada1a1900d753073",
      "f4d73e032be64ed79b16234e005c5254",
      "3c413c89cc294bb29d2ff11a9b3e0006",
      "3df31a5100f747e2afb9e275735bc58c",
      "04e0cd9f05a94f43968982700d9d57aa",
      "afdb691f382b4284bda25fcf192fe831",
      "7371b8e20efc47ebbf9fa3ed1e3347bb",
      "93bf9106c22b4b1cac66b28958694969",
      "cd4a4b8240ed467ea07cffb63ea86582",
      "30b0723d40594233b5b612fb24170ea4",
      "3f5ea4a732b847149d9c49aed96ee99a",
      "bbcc8caadd094fc0a54a21d49ddbb73a",
      "cfae6b2988e0495788966462e85418e1",
      "63953d0911b3419cbea02bea2c401bde",
      "6ad832cd6f8b41f3839dcfcedee5b799"
     ]
    },
    "id": "89bF-_iA5dkm",
    "outputId": "61904282-d966-48da-f770-b539e73c7e19"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_checkpoint = \"facebook/esm2_t33_650M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModel.from_pretrained(model_checkpoint)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset from Glycowork and Converting Sequences to Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "jbPdkmnd6g5-",
    "outputId": "1af64661-ca6e-4c84-9a18-9e7d1a27bd42"
   },
   "outputs": [],
   "source": [
    "from glycowork.glycan_data.loader import glycan_binding\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# Fetch all unique protein targets\n",
    "all_prots = list(set(glycan_binding.target.tolist()))\n",
    "batch_size = 10\n",
    "\n",
    "# Path to save the pickle file\n",
    "filename = '/content/drive/My Drive/esm2_embeddings_full.pkl'\n",
    "\n",
    "# Function to process a batch of proteins\n",
    "def process_batch(proteins):\n",
    "    inputs = tokenizer(proteins, padding=True, max_length=1000, truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    return {prot: last_hidden_states[i, 1:len(prot) + 1].mean(0).cpu().numpy().tolist() for i, prot in enumerate(proteins)}\n",
    "\n",
    "# Iterate over the proteins in batches and save the results\n",
    "with open(filename, 'ab') as file:  # 'ab' for appending in binary mode\n",
    "    for i in range(0, len(all_prots), batch_size):\n",
    "        batch_prots = all_prots[i:i + batch_size]\n",
    "        prot_dic = process_batch(batch_prots)\n",
    "        pickle.dump(prot_dic, file)\n",
    "\n",
    "print(\"All data processed and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Saved Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "gPx0BqpZGlnJ",
    "outputId": "43099af6-2b3c-452d-ad02-0a3f66d3a528"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# Path to where embeddings are stored\n",
    "filename = '/content/drive/My Drive/esm2_embeddings_full.pkl'\n",
    "\n",
    "# Initialize an empty dictionary to hold all the data\n",
    "prot_dic = {}\n",
    "\n",
    "# Open the pickle file and read from it\n",
    "with open(filename, 'rb') as file:\n",
    "    while True:\n",
    "        try:\n",
    "            # Load the data from the file and update the main dictionary\n",
    "            data = pickle.load(file)\n",
    "            prot_dic.update(data)\n",
    "        except EOFError:\n",
    "            # End of file reached\n",
    "            break\n",
    "\n",
    "# Check the loaded data\n",
    "print(f\"Loaded data for {len(prot_dic)} proteins.\")\n",
    "\n",
    "# Optionally, print some of the data to verify\n",
    "for protein, embedding in list(prot_dic.items())[:5]:\n",
    "    print(protein, embedding[:10])  # Print first 10 elements of each protein embedding for brevity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1392 glycan binding proteins in the glycan binding array dataset. These are the set of known existing glycan binding proteins. The question is: **amongst these known proteins, which ones have the highest affinity for sialyl-lewis-A?** Answering this question will form a starting point to prototype the nanobiosensor before further optimisation of the signal. This is just to form the proof of concept and MVP.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdxXH_erGtf_",
    "outputId": "b342b106-08fd-42fc-d8b3-4f6e3b2e94cb"
   },
   "outputs": [],
   "source": [
    "len(prot_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LectinOracle to predict Binding Affinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2-Dcc2MNJxmT",
    "outputId": "eb1d0293-ed7c-4e55-8c62-ef4a7d96a7bf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from glycowork.glycan_data.loader import lib, unwrap, build_custom_df, df_glycan, glycan_binding\n",
    "from glycowork.ml.processing import dataset_to_dataloader\n",
    "from glycowork.motif.tokenization import prot_to_coded\n",
    "\n",
    "# making predictions for human glycans\n",
    "df_species = build_custom_df(df_glycan, 'df_species')\n",
    "glyc = df_species.loc[df_species.Species == 'Homo_sapiens'].glycan.tolist()\n",
    "glyc = list(set(glyc))\n",
    "\n",
    "# making predictions for disease state glyc\n",
    "df_disease = build_custom_df(df_glycan, 'df_disease')\n",
    "df_panc = df_disease[df_disease['disease_association'] == 'pancreatic_cancer']\n",
    "df_panc = df_panc.drop_duplicates(subset = 'glycan')\n",
    "biomarkers = df_panc.glycan.tolist()\n",
    "\n",
    "# Choosing the right computing architecture\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load model\n",
    "from glycowork.ml.models import prep_model\n",
    "\n",
    "LeOr = prep_model('LectinOracle', 1, trained = True)\n",
    "LeOr.to(device)\n",
    "# LeOr_flex = prep_model('LectinOracle_flex', 1, trained = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hnp995AyVr5C"
   },
   "outputs": [],
   "source": [
    "def get_multi_pred(prot, glycans, model, prot_dic=prot_dic,\n",
    "                   background_correction = False, correction_df = None,\n",
    "                   batch_size = 128, libr = None, flex = False, mode = 'rep'):\n",
    "  \"\"\"Inner function to actually get predictions for lectin-glycan binding from LectinOracle-type model\\n\n",
    "  | Arguments:\n",
    "  | :-\n",
    "  | prot (string): protein amino acid sequence\n",
    "  | glycans (list): list of glycans in IUPACcondensed\n",
    "  | model (PyTorch object): trained LectinOracle-type model\n",
    "  | prot_dic (dictionary): dictionary of type protein sequence:ESM1b representation\n",
    "  | background_correction (bool): whether to correct predictions for background; default:False\n",
    "  | correction_df (dataframe): background prediction for (ideally) all provided glycans; default:None\n",
    "  | batch_size (int): change to batch_size used during training; default:128\n",
    "  | libr (dict): dictionary of form glycoletter:index\n",
    "  | flex (bool): depends on whether you use LectinOracle (False) or LectinOracle_flex (True); default:False\\n\n",
    "  | Returns:\n",
    "  | :-\n",
    "  | Returns dataframe of glycan sequences and predicted binding to prot\n",
    "  \"\"\"\n",
    "  if libr is None:\n",
    "      libr = lib\n",
    "  # Preparing dataset for PyTorch\n",
    "  if flex:\n",
    "      prot = prot_to_coded([prot])\n",
    "      feature = prot * len(glycans)\n",
    "  else:\n",
    "      rep = prot_dic.get(prot, \"new protein, no stored embedding\")\n",
    "      feature = [rep] * len(glycans)\n",
    "  train_loader = dataset_to_dataloader(glycans, [0.99]*len(glycans),\n",
    "                                         libr = libr, batch_size = batch_size,\n",
    "                                         shuffle = False, extra_feature = feature)\n",
    "  model = model.eval()\n",
    "  res = []\n",
    "  # Get predictions for each mini-batch\n",
    "  for k in train_loader:\n",
    "    x, y, edge_index, prot, batch = k.labels, k.y, k.edge_index, k.train_idx, k.batch\n",
    "    x, y, edge_index, prot, batch = x.to(device), y.to(device), edge_index.to(device), prot.view(max(batch) + 1, -1).float().to(device), batch.to(device)\n",
    "    pred, reps, _ = model(prot, x, edge_index, batch, inference=True)\n",
    "    if mode == 'rep':\n",
    "      res.append(reps)\n",
    "    else:\n",
    "      res.append(pred)\n",
    "  res = unwrap([res[k].detach().cpu().numpy() for k in range(len(res))])\n",
    "\n",
    "  return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted, if we store `prot_rep` (protein representations) from the pretrained `LectinOracle`, we will get a different set of embeddings that have been fine tuned during training on this glycan array dataset. \n",
    "\n",
    "Since we are only interested in the binding affinities of known glycan binding proteins (lectins), we will just use the set of embeddings obtained from `ESM2` directly but use the `LectinOracle` which combines ESM2 representations (so they have been fine tuned) to predict binding affinities. The assumption is that the model is good enough at prediction such that the relative ranking of affinities between ESM2 embeddings and fine-tuned ESM2 embeddings should be similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yEeE7dppQiBE",
    "outputId": "443a2fef-4344-44af-93db-a1f31b3bae05"
   },
   "outputs": [],
   "source": [
    "#getting learned protein represenations from LectinOracle\n",
    "# DO NOT run this cell, will overwrite already saved embeddings, this takes a long time to execute\n",
    "prot_reps = {}\n",
    "binding_pred = {}\n",
    "for i, seq in enumerate(glycan_binding.target.tolist()[:2]):\n",
    "  if(i%10 == 0):\n",
    "    print(i)\n",
    "  prot_reps[seq] = np.array(get_multi_pred(seq, [glyc[0]], LeOr)).squeeze()\n",
    "  binding_pred[seq] = unwrap(get_multi_pred(seq, glyc, LeOr, mode='pred'))\n",
    "\n",
    "# Uncomment lines below to get the fine-tuned embedding representations which happen to be stored by the model during its training for convenient access. Note however THIS TAKES A LONG TIME TO RUN! I have run this once already and saved the data.\n",
    "# pd.DataFrame(prot_reps).T.to_csv('/content/drive/My Drive/LeOr_embeddings.csv')\n",
    "# motif_pred = pd.DataFrame(binding_pred).T\n",
    "# motif_pred.columns = glyc\n",
    "# motif_pred.to_csv('/content/drive/My Drive/human_glycan_motif_predicted.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of Lectin Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding vector which is in $\\mathbb{R}^{128}$ must be projected to $\\mathbb{R}^2$ for visualisation. We use tSNE to do this which assumes that local distances between points are conserved. Therefore, although the visualisation may appear different each time, relative distance between points should be preserved (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). If we then use colours to indicate strength of binding affinity, we should see dense regions of colour representing the family of lectins that have high affinity for the glycan epitope of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that `scikit-learn`, `matplotlibt`, `seaborn`, `pandas`, `numpy` are all installed as dependencies if working locally. I produced these plots in Google Colab so these were preinstalled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VLfCl82Va7F_",
    "outputId": "e8955008-aa6e-4e40-ed93-6efafcc6050f"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "df_prot_reps = pd.read_csv('/content/drive/My Drive/LeOr_embeddings.csv', index_col=0)\n",
    "\n",
    "df_pred = pd.read_csv('/content/drive/My Drive/human_glycan_motif_predicted.csv', index_col=0)\n",
    "\n",
    "# visualisation of 128 dimensional LeOr representations using TSNE\n",
    "\n",
    "motif_pred_dict= df_pred.to_dict()\n",
    "glyc = list(df_pred.columns)\n",
    "\n",
    "X = df_prot_reps.values\n",
    "X_embedded = TSNE(random_state=42).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell when working in a new session for just visualisation\n",
    "# !pip install glycowork\n",
    "# !pip install 'glycowork[draw]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines the motif of interest (aka. glycan epitope Sialyl-Lewis-A). This is the motif that is present on CA19-9 (the current gold-standard biomarker for diagnosis of pancreatic cancer). HOWEVER, it is important to note that the glycan motif is not unique to CA19-9. In fact, glycosylation is a general post-translational modification of proteins. The question is can we still detect upregulated levels of CA19-9 using upregulated sLeA as a proxy? We would need to test the specificity of the biosensor with other biomolecules that have similar structures to sLeA and CA19-9. The line and definition of what is CA19-9 starts becoming blurry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "s0R9aLlWDJMZ",
    "outputId": "91f9d09a-f2a8-4d55-b0db-d58220838e44"
   },
   "outputs": [],
   "source": [
    "from glycowork.motif.draw import GlycoDraw\n",
    "from glycowork.glycan_data.loader import unwrap\n",
    "\n",
    "# the glycan structure of Sialyl-Lewis-A\n",
    "motif='Neu5Ac(a2-3)Gal(b1-3)[Fuc(a1-4)]GlcNAc(b1-3)Gal'\n",
    "\n",
    "if motif in df_pred.columns:\n",
    "  binding_val = list(motif_pred_dict[motif].values())\n",
    "else:\n",
    "  print('motif not in precomputed file')\n",
    "  binding_val = unwrap([unwrap(get_multi_pred(seq, [motif], LeOr, mode='pred')) for seq in list(df_pred.index)])\n",
    "\n",
    "GlycoDraw(motif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "AlepaKLbKbRF",
    "outputId": "347dbdb3-3e8b-4c9f-de6f-5a1a9ab001e2"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (12, 9))\n",
    "\n",
    "sns.scatterplot(x = X_embedded[:,0], y = X_embedded[:,1], s = 50, alpha = 0.6,\n",
    "                hue = binding_val, palette = 'Greens')\n",
    "plt.legend(loc = 'center left', bbox_to_anchor = (1, 0.5))\n",
    "plt.xlabel('t-SNE Dim1')\n",
    "plt.ylabel('t-SNE Dim2')\n",
    "plt.title(motif)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we see a cluster of lectins that have high specificity for sLeA. We will use these a starting a point for experimentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tx8MhkQ5SKb3"
   },
   "source": [
    "# References \n",
    "\n",
    "Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., Verkuil, R., Kabeli, O., Shmueli, Y., dos Santos Costa, A., Fazel-Zarandi, M., Sercu, T., Candido, S., Rives, A., 2023. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science 379, 1123–1130. https://doi.org/10.1126/science.ade2574\n",
    "\n",
    "Lundstrøm, J., Korhonen, E., Lisacek, F., Bojar, D., 2022. LectinOracle: A Generalizable Deep Learning Model for Lectin–Glycan Binding Prediction. Advanced Science 9, 2103807. https://doi.org/10.1002/advs.202103807\n",
    "\n",
    "Bojar, D., Lisacek, F., 2022. Glycoinformatics in the Artificial Intelligence Era. Chem. Rev. 122, 15971–15988. https://doi.org/10.1021/acs.chemrev.2c00110\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMQ1L4RgCgBoa8pR9n7QnTI",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
